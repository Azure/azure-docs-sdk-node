### YamlMime:UniversalReference
items:
  - uid: '@azure/search-documents.CustomAnalyzer'
    name: CustomAnalyzer
    fullName: CustomAnalyzer
    children:
      - '@azure/search-documents.CustomAnalyzer.charFilters'
      - '@azure/search-documents.CustomAnalyzer.name'
      - '@azure/search-documents.CustomAnalyzer.odatatype'
      - '@azure/search-documents.CustomAnalyzer.tokenFilters'
      - '@azure/search-documents.CustomAnalyzer.tokenizerName'
    langs:
      - typeScript
    type: interface
    summary: >-
      Allows you to take control over the process of converting text into
      indexable/searchable tokens.

      It's a user-defined configuration consisting of a single predefined
      tokenizer and one or more

      filters. The tokenizer is responsible for breaking text into tokens, and
      the filters for

      modifying tokens emitted by the tokenizer.
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.CustomAnalyzer.charFilters'
    name: charFilters
    fullName: charFilters
    children: []
    langs:
      - typeScript
    type: property
    summary: >-
      A list of character filters used to prepare input text before it is
      processed by the

      tokenizer. For instance, they can replace certain characters or symbols.
      The filters are run

      in the order in which they are listed.
    optional: true
    syntax:
      content: 'charFilters?: string[]'
      return:
        type:
          - 'string[]'
        description: ''
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.CustomAnalyzer.name'
    name: name
    fullName: name
    children: []
    langs:
      - typeScript
    type: property
    summary: >-
      The name of the analyzer. It must only contain letters, digits, spaces,
      dashes or underscores,

      can only start and end with alphanumeric characters, and is limited to 128
      characters.
    syntax:
      content: 'name: string'
      return:
        type:
          - string
        description: ''
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.CustomAnalyzer.odatatype'
    name: odatatype
    fullName: odatatype
    children: []
    langs:
      - typeScript
    type: property
    summary: Polymorphic Discriminator
    syntax:
      content: 'odatatype: "#Microsoft.Azure.Search.CustomAnalyzer"'
      return:
        type:
          - '"#Microsoft.Azure.Search.CustomAnalyzer"'
        description: ''
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.CustomAnalyzer.tokenFilters'
    name: tokenFilters
    fullName: tokenFilters
    children: []
    langs:
      - typeScript
    type: property
    summary: >-
      A list of token filters used to filter out or modify the tokens generated
      by a tokenizer. For

      example, you can specify a lowercase filter that converts all characters
      to lowercase. The

      filters are run in the order in which they are listed.
    optional: true
    syntax:
      content: 'tokenFilters?: string[]'
      return:
        type:
          - 'string[]'
        description: ''
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.CustomAnalyzer.tokenizerName'
    name: tokenizerName
    fullName: tokenizerName
    children: []
    langs:
      - typeScript
    type: property
    summary: >-
      The name of the tokenizer to use to divide continuous text into a sequence
      of tokens, such as

      breaking a sentence into words. KnownTokenizerNames is an enum containing
      known values.
    syntax:
      content: 'tokenizerName: string'
      return:
        type:
          - string
        description: ''
    package: '@azure/search-documents'
