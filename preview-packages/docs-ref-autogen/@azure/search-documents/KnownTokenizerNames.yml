### YamlMime:UniversalReference
items:
  - uid: '@azure/search-documents.KnownTokenizerNames'
    name: KnownTokenizerNames
    fullName: KnownTokenizerNames
    children:
      - '@azure/search-documents.KnownTokenizerNames.Classic'
      - '@azure/search-documents.KnownTokenizerNames.EdgeNGram'
      - '@azure/search-documents.KnownTokenizerNames.Keyword'
      - '@azure/search-documents.KnownTokenizerNames.Letter'
      - '@azure/search-documents.KnownTokenizerNames.Lowercase'
      - >-
        @azure/search-documents.KnownTokenizerNames.MicrosoftLanguageStemmingTokenizer
      - '@azure/search-documents.KnownTokenizerNames.MicrosoftLanguageTokenizer'
      - '@azure/search-documents.KnownTokenizerNames.NGram'
      - '@azure/search-documents.KnownTokenizerNames.PathHierarchy'
      - '@azure/search-documents.KnownTokenizerNames.Pattern'
      - '@azure/search-documents.KnownTokenizerNames.Standard'
      - '@azure/search-documents.KnownTokenizerNames.UaxUrlEmail'
      - '@azure/search-documents.KnownTokenizerNames.Whitespace'
    langs:
      - typeScript
    type: enum
    summary: Defines values for TokenizerName.
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Classic'
    name: Classic
    children: []
    langs:
      - typeScript
    summary: >-
      Grammar-based tokenizer that is suitable for processing most
      European-language documents. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.EdgeNGram'
    name: EdgeNGram
    children: []
    langs:
      - typeScript
    summary: >-
      Tokenizes the input from an edge into n-grams of the given size(s). See

      https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Keyword'
    name: Keyword
    children: []
    langs:
      - typeScript
    summary: >-
      Emits the entire input as a single token. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Letter'
    name: Letter
    children: []
    langs:
      - typeScript
    summary: >-
      Divides text at non-letters. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Lowercase'
    name: Lowercase
    children: []
    langs:
      - typeScript
    summary: >-
      Divides text at non-letters and converts them to lower case. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: >-
      @azure/search-documents.KnownTokenizerNames.MicrosoftLanguageStemmingTokenizer
    name: MicrosoftLanguageStemmingTokenizer
    children: []
    langs:
      - typeScript
    summary: >-
      Divides text using language-specific rules and reduces words to their base
      forms.
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.MicrosoftLanguageTokenizer'
    name: MicrosoftLanguageTokenizer
    children: []
    langs:
      - typeScript
    summary: Divides text using language-specific rules.
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.NGram'
    name: NGram
    children: []
    langs:
      - typeScript
    summary: >-
      Tokenizes the input into n-grams of the given size(s). See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.PathHierarchy'
    name: PathHierarchy
    children: []
    langs:
      - typeScript
    summary: >-
      Tokenizer for path-like hierarchies. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Pattern'
    name: Pattern
    children: []
    langs:
      - typeScript
    summary: >-
      Tokenizer that uses regex pattern matching to construct distinct tokens.
      See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Standard'
    name: Standard
    children: []
    langs:
      - typeScript
    summary: >-
      Standard Lucene analyzer; Composed of the standard tokenizer, lowercase
      filter and stop

      filter. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.UaxUrlEmail'
    name: UaxUrlEmail
    children: []
    langs:
      - typeScript
    summary: >-
      Tokenizes urls and emails as one token. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
  - uid: '@azure/search-documents.KnownTokenizerNames.Whitespace'
    name: Whitespace
    children: []
    langs:
      - typeScript
    summary: >-
      Divides text at whitespace. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html
    type: field
    numericValue: null
    package: '@azure/search-documents'
